"""Ollama API client with vision model support."""

import base64
import logging
import time
from pathlib import Path

from ollama import Client
from PIL import Image

logger = logging.getLogger(__name__)


def encode_image_base64(image_path: Path) -> str:
    """Encode an image to base64.

    Args:
        image_path: Path to the image.

    Returns:
        The base64 encoded image.
    """
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")


def describe_image(
    image_path: Path,
    model: str,
    system_prompt: str,
    temperature: float = 0.7,
    num_ctx: int = 8192,
    ollama_host: str | None = None,
) -> str:
    """Generate a description of an image via Ollama.

    Args:
        image_path: Path to the image to describe.
        model: Name of the Ollama model to use.
        system_prompt: System prompt to guide the description.
        temperature: Temperature for generation.
        num_ctx: Context window size (important for large images).
        ollama_host: Ollama server URL (optional).

    Returns:
        The description generated by the model.

    Raises:
        ollama.ResponseError: On Ollama API error.
    """
    logger.debug(f"Encoding image: {image_path.name}")
    encode_start = time.time()

    # Get image dimensions for token estimation
    with Image.open(image_path) as img:
        width, height = img.size
        # Estimate tokens: most vision models use ~14x14 patches after resize
        # Rough estimate based on common vision encoders
        max_dim = max(width, height)
        if max_dim > 1280:
            scale = 1280 / max_dim
            est_width, est_height = int(width * scale), int(height * scale)
        else:
            est_width, est_height = width, height
        patches = (est_width // 14) * (est_height // 14)
        est_tokens = patches + 100  # +100 for special tokens overhead

    image_data = encode_image_base64(image_path)
    encode_time = time.time() - encode_start
    logger.debug(
        f"Image encoded in {encode_time:.2f}s - "
        f"size: {width}x{height}, ~{est_tokens} tokens estimated, "
        f"{len(image_data)} bytes base64"
    )

    host_info = ollama_host or "default (localhost:11434)"
    logger.debug(f"Creating Ollama client (host: {host_info})")
    client = Client(host=ollama_host) if ollama_host else Client()

    logger.info(f"Sending request to Ollama model '{model}' for: {image_path.name} (ctx: {num_ctx})")
    api_start = time.time()

    response = client.chat(
        model=model,
        messages=[
            {
                "role": "system",
                "content": system_prompt,
            },
            {
                "role": "user",
                "content": "Describe this image.",
                "images": [image_data],
            }
        ],
        options={"temperature": temperature, "num_ctx": num_ctx},
    )

    api_time = time.time() - api_start
    response_content = response["message"]["content"]
    logger.info(f"Response received in {api_time:.2f}s (length: {len(response_content)} chars)")
    logger.debug(f"Response preview: {response_content[:100]}...")

    return response_content
